[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topics in Causal Inference",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "chapters/misc.html",
    "href": "chapters/misc.html",
    "title": "1  Non-Parametric Efficiency Theory",
    "section": "",
    "text": "flowchart LR\n  HinesDukesDiazOrdaz2022[Hines et al. 2022] --&gt; chernozhukov2018a[Chernozhukov et al. 2018]\n\n\n\n\n\nFrom the abstract of (Hines et al. 2022):\n\nEvaluation of treatment effects and more general estimands is typically achieved via parametric modelling, which is unsatisfactory since model misspecification is likely. Data-adaptive model building (e.g. statistical/machine learning) is commonly employed to reduce the risk of misspecification. Naive use of such methods, however, delivers estimators whose bias may shrink too slowly with sample size for inferential methods to perform well, including those based on the bootstrap. Bias arises because standard data-adaptive methods are tuned towards minimal prediction error as opposed to e.g. minimal MSE in the estimator. This may cause excess variability that is difficult to acknowledge, due to the complexity of such strategies. Building on results from nonparametric statistics, targeted learning and debiased machine learning overcome these problems by constructing estimators using the estimand’s efficient influence function under the nonparametric model. These increasingly popular methodologies typically assume that the efficient influence function is given, or that the reader is familiar with its derivation. In this paper, we focus on derivation of the efficient influence function and explain how it may be used to construct statistical/machine-learning-based estimators. We discuss the requisite conditions for these estimators to perform well and use diverse examples to convey the broad applicability of the theory.\n\n\n\n\n\nHines, Oliver, Oliver Dukes, Karla Diaz-Ordaz, and Stijn Vansteelandt. 2022. “Demystifying Statistical Learning Based on Efficient Influence Functions.” The American Statistician 76 (3): 292–304. https://doi.org/10.1080/00031305.2021.2021984."
  },
  {
    "objectID": "chapters/dr.html",
    "href": "chapters/dr.html",
    "title": "2  Debiased Learning",
    "section": "",
    "text": "flowchart LR\n  chernozhukov2018a[Chernozhukov et al. 2018]\n\n\n\n\n\nFrom the abstract of (Chernozhukov et al. 2018):\n\nWe revisit the classic semi-parametric problem of inference on a low-dimensional parameter \\(\\theta_0\\) in the presence of high-dimensional nuisance parameters \\(\\eta_0\\). We depart from the classical setting by allowing for \\(\\eta_0\\) to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate \\(\\eta_0\\), we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating \\(\\eta_0\\) cause a heavy bias in estimators of \\(\\theta_0\\) that are obtained by naively plugging ML estimators of \\(\\eta_0\\) into estimating equations for \\(\\theta_0\\). This bias results in the naive estimator failing to be \\(N^{−1/2}\\) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest \\(\\theta_0\\) can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate \\(\\theta_0\\); (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an \\(N^{−1/2}\\)-neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.\n\n\n\n\n\nChernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” The Econometrics Journal 21: 1–68. https://doi.org/10.1111/ectj.12097."
  },
  {
    "objectID": "chapters/mtp.html#estimation",
    "href": "chapters/mtp.html#estimation",
    "title": "3  Modified Treatment Policy",
    "section": "3.1 Estimation",
    "text": "3.1 Estimation\nFrom the abstract of (van der Laan and Dudoit 2003):\nFrom the abstract of (Rubin and Laan 2005):\nFrom the abstract of (Rubin and Laan 2007):"
  },
  {
    "objectID": "chapters/mtp.html#continuous-treatments",
    "href": "chapters/mtp.html#continuous-treatments",
    "title": "3  Modified Treatment Policy",
    "section": "3.2 Continuous treatments",
    "text": "3.2 Continuous treatments\nFrom the abstract of (Gill and Robins 2001):\nFrom the abstract of (Kennedy et al. 2017):\n\nContinuous treatments (e.g. doses) arise often in practice, but many available causal effect estimators are limited by either requiring parametric models for the effect curve, or by not allowing doubly robust covariate adjustment. We develop a novel kernel smoothing approach that requires only mild smoothness assumptions on the effect curve and still allows for misspecification of either the treatment density or outcome regression. We derive asymptotic properties and give a procedure for data-driven bandwidth selection. The methods are illustrated via simulation and in a study of the effect of nurse staffing on hospital readmissions penalties.\n\nFrom the abstract of (I. Díaz and Laan 2013):\nFrom the abstract of (Iván Díaz et al. 2023):\n\n\n\n\nDíaz, I., and M. J. Laan. 2013. “Targeted Data Adaptive Estimation of the Causal Dose Curve.” Journal of Causal Inference 1: 171–92.\n\n\nDíaz, Iván, Nicholas Williams, Katherine L. Hoffman, and Edward J. Schenck. 2023. “Nonparametric Causal Effects Based on Longitudinal Modified Treatment Policies.” Journal of the American Statistical Association 118 (542): 846–57. https://doi.org/10.1080/01621459.2021.1955691.\n\n\nGill, R. D., and J. M. Robins. 2001. “Causal Inference for Complex Longitudinal Data: The Continuous Case.” Ann. Statist 29: 1785–1811.\n\n\nKennedy, Edward H., Zongming Ma, Matthew D. McHugh, and Dylan S. Small. 2017. “Non-Parametric Methods for Doubly Robust Estimation of Continuous Treatment Effects.” Journal of the Royal Statistical Society Series B: Statistical Methodology 79 (4): 1229–45. https://doi.org/10.1111/rssb.12212.\n\n\nRubin, D., and M. J. Laan. 2005. “A General Imputation Methodology for Nonparametric Regression with Censored Data.” Technical Report 194,. Berkeley Division of Biostatistics Working Paper Series. U.C.\n\n\n———. 2007. “A Doubly Robust Censoring Unbiased Transformation.” The International Journal of Biostatistics 3.\n\n\nvan der Laan, Mark, and Sandrine Dudoit. 2003. “Unified Cross-Validation Methodology For Selection Among Estimators and a General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities and Examples.” U.C. Berkeley Division of Biostatistics Working Paper Series, November."
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W.\nNewey, and J. Robins. 2018. “Double/Debiased Machine\nLearning for Treatment and Structural Parameters.” The\nEconometrics Journal 21: 1–68. https://doi.org/10.1111/ectj.12097.\n\n\nDíaz, I., and M. J. Laan. 2013. “Targeted Data Adaptive Estimation\nof the Causal Dose Curve.” Journal of Causal\nInference 1: 171–92.\n\n\nDíaz, Iván, Nicholas Williams, Katherine L. Hoffman, and Edward J.\nSchenck. 2023. “Nonparametric Causal Effects Based on\nLongitudinal Modified Treatment Policies.”\nJournal of the American Statistical Association 118 (542):\n846–57. https://doi.org/10.1080/01621459.2021.1955691.\n\n\nGill, R. D., and J. M. Robins. 2001. “Causal Inference for Complex\nLongitudinal Data: The Continuous Case.” Ann. Statist\n29: 1785–1811.\n\n\nHines, Oliver, Oliver Dukes, Karla Diaz-Ordaz, and Stijn Vansteelandt.\n2022. “Demystifying Statistical Learning Based on\nEfficient Influence Functions.” The American\nStatistician 76 (3): 292–304. https://doi.org/10.1080/00031305.2021.2021984.\n\n\nKennedy, Edward H., Zongming Ma, Matthew D. McHugh, and Dylan S. Small.\n2017. “Non-Parametric Methods for Doubly Robust\nEstimation of Continuous Treatment Effects.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 79 (4): 1229–45. https://doi.org/10.1111/rssb.12212.\n\n\nRubin, D., and M. J. Laan. 2005. “A General Imputation Methodology\nfor Nonparametric Regression with Censored Data.” Technical\nReport 194,. Berkeley Division of Biostatistics Working Paper Series.\nU.C.\n\n\n———. 2007. “A Doubly Robust Censoring Unbiased\nTransformation.” The International Journal of\nBiostatistics 3.\n\n\nvan der Laan, Mark, and Sandrine Dudoit. 2003. “Unified\nCross-Validation Methodology For Selection Among Estimators\nand a General Cross-Validated Adaptive Epsilon-Net\nEstimator: Finite Sample Oracle Inequalities and\nExamples.” U.C. Berkeley Division of\nBiostatistics Working Paper Series, November."
  },
  {
    "objectID": "chapters/efficiency.html",
    "href": "chapters/efficiency.html",
    "title": "1  Non-Parametric Efficiency Theory",
    "section": "",
    "text": "flowchart LR\n  HinesDukesDiazOrdaz2022[Hines et al. 2022] --&gt; chernozhukov2018a[Chernozhukov et al. 2018]\n\n\n\n\n\nFrom the abstract of (Hines et al. 2022):\n\nEvaluation of treatment effects and more general estimands is typically achieved via parametric modelling, which is unsatisfactory since model misspecification is likely. Data-adaptive model building (e.g. statistical/machine learning) is commonly employed to reduce the risk of misspecification. Naive use of such methods, however, delivers estimators whose bias may shrink too slowly with sample size for inferential methods to perform well, including those based on the bootstrap. Bias arises because standard data-adaptive methods are tuned towards minimal prediction error as opposed to e.g. minimal MSE in the estimator. This may cause excess variability that is difficult to acknowledge, due to the complexity of such strategies. Building on results from nonparametric statistics, targeted learning and debiased machine learning overcome these problems by constructing estimators using the estimand’s efficient influence function under the nonparametric model. These increasingly popular methodologies typically assume that the efficient influence function is given, or that the reader is familiar with its derivation. In this paper, we focus on derivation of the efficient influence function and explain how it may be used to construct statistical/machine-learning-based estimators. We discuss the requisite conditions for these estimators to perform well and use diverse examples to convey the broad applicability of the theory.\n\n\n\n\n\nHines, Oliver, Oliver Dukes, Karla Diaz-Ordaz, and Stijn Vansteelandt. 2022. “Demystifying Statistical Learning Based on Efficient Influence Functions.” The American Statistician 76 (3): 292–304. https://doi.org/10.1080/00031305.2021.2021984."
  }
]